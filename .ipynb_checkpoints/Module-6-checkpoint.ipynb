{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Latency Hiding Techniques\n",
    "THe basic problem is that, in a multiprocessor system the memory accesses takes a lot of time. And compared to a processor, the memory access time is less compared to the improvements in processor speeds. So __latency hiding__ techniques are used to get better results.\n",
    "#### Techniques\n",
    "- Pre Fetching Techniques\n",
    "- Using Coherent Caches\n",
    "- Using Relaxed Memory\n",
    "- Using multiple contexts support\n",
    "\n",
    "#### 1.1 Shared Virtual Memory\n",
    "##### The problem:\n",
    "- Usually in a __NUMA__ system. there will be shared memory. And these shared memory will be accessed by different addresses. This causes delays when updating the caches and invalidating the copies, etc.\n",
    "- In this approach, cache coherence was maintained using distributed directory based protocol. \n",
    "- For each memory block, a directory kept track of all the nodes accessing it. Load and writes were done with write buffers. \n",
    "##### The solution:\n",
    "- The solution to this problem is to use a __shared virtual memory__. \n",
    "- Instead of having seperate addresses for the shared memory.\n",
    "- A __virtual address__ was used to map to the shared memory.\n",
    "- A memory location having read access was allowed to be held by the different processors.\n",
    "- A memory location to be written was only allowed to be held by the processor writing it.\n",
    "- When a page fault occurs, the memory manager retrieves the memory location and brings it to the processors cache.\n",
    "- This allows the code to be larger in size by storing it in continuous virtual address space.\n",
    "#### 1.2 Prefetching Techniques\n",
    "##### Overvew\n",
    "- It is the technique which brings the next required data before the processor actually wants it.\n",
    "- There are two types of prefetches:\n",
    "    - __Binding__ : In this prefetching, the data is loaded ultimately and thus has the risk of it getting stale when other processors write over that data. This has a risk if the data is changed, thus forcing the data to be read again and then wasting more clock cycles.\n",
    "    - __Non Binding__ : In this type of prefetching, the data comes closer to the processor but its not loaded entirely so that it can be loaded when required with the updated changes.\n",
    "- __Hardware controlled__ prefetching includes __long cache lines__ and __look ahead__. It is faster. \n",
    "- __Software controlled__ prefetching lets us write exclusive instructions which will help prefetching more customizable. However there is the additional over. head of instructions.\n",
    "##### Benefits:\n",
    "- It speeds up the execution greatly.\n",
    "- Back to back issuing of prefetch can help in hiding all but the first prefetch instruction due to pipelining.\n",
    "- Ownership based prefetching can solve the coherence problem.\n",
    "#### 1.3 Distributed Coherent Caches\n",
    "##### Problem:\n",
    "- Eventhough snoopy based protocols are very effective when it comes to single processor systems, etc. It gets really complicated in multi processor systems.\n",
    "- Earlier multiprocessor systems avoided caches because of this. However, it slowed up execution a lot.\n",
    "- Largest saving in latency was made in reducing the cycles for reading from memory. However, that power is lost if we avoid caching.\n",
    "\n",
    "#### 1.4 Scalable Coherence Interface\n",
    "##### Solution:\n",
    "- A __Scalable Coherence Inteface__ with low latency is used to connect between the nodes instead of a traditional bused backplane.\n",
    "- Connects the interface between nodes and external interconnect using 16-bit links with speeds of up to 1 GBps.\n",
    "- Each node has an input to output link which connects to an SCI ring or a crossbar.\n",
    "- Instead of broadcast in the case of a bus, point to point communication is used.\n",
    "- The bandwidth, arbitration and the addressing outperforms the latter.\n",
    "- Since there is no snoopy controller per node, it is also very less expensive.\n",
    "- Although SCI is scallable, the memory in the cache directory also scales up.\n",
    "- The performance however does not scale.\n",
    "\n",
    "##### Working\n",
    "###### Sharing-List Structures\n",
    "- They are data structures that contain the data of  the shared location.\n",
    "- THey are dynamically created, pruned and destroyed.\n",
    "- They have the property to bypass the coherence protocols for locally cached data.\n",
    "- Communications are supported by shared memory controllers.\n",
    "- There is a first bit that tags the first processor.\n",
    "- The other processors are linked using doubly linked list.\n",
    "###### Sharing-List Creation\n",
    "- The states of every memory are defined as __clean__, __dirty__, __valid__ or __stale__.\n",
    "- Head processor is always responsible for list management.\n",
    "- First the location is in the home state.\n",
    "- When a processor asks for it, the home state is changed to cached after making necessary changes to it.\n",
    "###### Sharing-List Updates\n",
    "- For subsequent memory requests, the memory is cached which makes the head of the sharing-list dirty.\n",
    "- When a request for that memory is made, instead of the pointer to the memory a pointer to the other node accessing it is given.\n",
    "- A second cache to cache transaction called __prepend__ takes place which makes the old point its backward pointer to the new one.\n",
    "- The newly accessed node becomes the new head.\n",
    "- A processor may release itself from the list.\n",
    "\n",
    "#### 1.5 Relaxed Memory Consistency\n",
    "###### Processor Consistency\n",
    "\n",
    "Writes of one processor is always in program order, however the writes of two different processors may not always be in program order.\n",
    "\n",
    "- The basic idea is that the idea of maintaining consistency is relaxed with respect to the read and write operations thereby allowing additional buffering and pipelining opportunities.\n",
    "- The following conditions allow the reads following a write to bypass the write:\n",
    "    - Before a __read__ is performed w:r:t any other processor, all previous reads are to be performed.\n",
    "    - Before a __write__ is performed w:r:t any other processor, all previous reads or writes are to be performed.\n",
    "###### Release Consistency\n",
    "The release consistency allows a relaxed way for releasing and acquiring locks thereby providing flexibility in buffering and pipelining.\n",
    "\n",
    "- Before a __read or write__ access is allowed to perform w:r:t any other processor all previous acquire must be performed.\n",
    "- Before a __release__ accesss is allowed to perform with respect to any other processor, all previous read and store must be performed.\n",
    "- __Special accesses__ are processor-consistent with one another. The ordering restrictions imposed by weak consistency are not present in release consistency. Instead, release consistency requires processor consistency and not sequential consistency.\n",
    "\n",
    "__Weak consistency__ :- programmer defined synchronisation to make the program more consistent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Multithreading Issuses and Solutions\n",
    "\n",
    "#### Parameters to analyze performance\n",
    "1. __The latency__\n",
    "2. __The number of threads__\n",
    "3. __The context-switching overhead__\n",
    "4. __The interval between switches__\n",
    "\n",
    "#### Problem of asynchrony\n",
    "\n",
    "When two threads read two seperate data, then the reading and processing by the induvidual two threads becomes fast. However, if another thread reads from the two threads then idiling will take place waiting for the other threads to complete.\n",
    "\n",
    "##### Multithreading Solutions\n",
    "- This solution involves using multiple threads to hide the latency.\n",
    "- For a load operation, when the data is being read another thread can begin working thereby saving time on waiting.\n",
    "- However, the other thread switched must not have a load themsevles. This will make the situation worse.\n",
    "- Using __continuations__. Each load operation is assigned with an appropriate operation that does not have this issue.\n",
    "##### Distributed cache\n",
    "- The distributed cache involves every node owning a cache location.\n",
    "- Each location has an __import__ and an __export__ list. They keep track of who all the ones they are sharing and the ones shared with them.\n",
    "- This provides a solution for remote loads, not for synchronizing loads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Multiple context processors\n",
    "\n",
    "The basic idea is to improve the efficiency by increasing the __busy__ time over __busy__+__switching__+__idle__.\n",
    "\n",
    "###### Context Switching Policies\n",
    "- __Switch on Cache miss__ - The context is switched when a cache is missed.\n",
    "- __Switch on every load__ - The context is switched to a useful process when waiting for the other context to load.\n",
    "- __Switch on every instruction__ - Despite being a load operation, every instruction has some delay. To make up for that, the context is switched.\n",
    "- __Switch on block of instruction__ - Block of instruction from different threads are interleaved. Thus switching will help in improving the cache hit ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Fine Gain Paralleism"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
